{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Learning10_앙상블구조.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMs_4cT5YSlD"
      },
      "source": [
        "# **랜덤 포레스트**\n",
        "랜덤 포레스트:랜덤서치를 활용해 학습시킨 여러대의 트리 모델을 활용하여 하나의 결과를 뽑아내는 앙상블구조"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rV-R59aSiLQ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "wine = pd.read_csv('https://bit.ly/wine_csv_data')\n",
        "\n",
        "data = wine[['alcohol', 'sugar', 'pH']].to_numpy()\n",
        "target = wine['class'].to_numpy()\n",
        "train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca8wAd_fYrmP",
        "outputId": "b9c4ef49-3384-4a72-b0b7-9a956392f2d3"
      },
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "#sklearn.ensemble패키지 하위의 랜덤 포레스트 모델인 RandomForestClassifier클래스를 사용한다.\n",
        "#이 클래스는 기본적으로 100개의 트리를 학습시킨다.\n",
        "\n",
        "#RandomForestClassifier의 매개변수 n_jobs는 랜덤포래스트 모델이 여러개의 트리 모델을 학습시킬때 몇개의 코어를 사용할지 지정한다.\n",
        "#n_jobs=-1일 경우 사용가능한 모든 코어를 사용하여 훈련시킨다.\n",
        "rf = RandomForestClassifier(n_jobs=-1)\n",
        "#return_train_score:True로 설정하면 cross_validate메서드의 반환결과 딕셔너리에 'train_score':(훈련세트 점수) 쌍을 추가해준다.\n",
        "scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9978352895600912 0.8910864736803139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4l8fqqHaxqh",
        "outputId": "3e05d589-9529-4a08-f516-320c1af6aee6"
      },
      "source": [
        "rf.fit(train_input, train_target)\n",
        "#feature_importances:특성 중요도\n",
        "#[alchor sugar pH]\n",
        "print(rf.feature_importances_)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.23138395 0.50619094 0.26242511]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B79InV9jgn4A",
        "outputId": "543777c3-7f81-4631-9bbf-35af03fe69a9"
      },
      "source": [
        "#oob_score:True 랜덤 포레스트 모델의 oob(Out Of Bag)으로 검증한 점수를 도출한다.\n",
        "#*oob(Out Of Bag):랜덤 포레스트 모델은 훈련세트에서 균등 분포 중복 샘플링(중복 조합)을 여러번 실행하여 여러개의 훈련세트로 분할하며,\n",
        "#                 이 분할한 훈련세트들로 각각의 트리 모델을 학습시킨다. 이 때문에, 학습을 한 번 시킬때 마다 남는 요소들의 집합이 생기게 되며,\n",
        "#                 n번 학습시킬 경우 n개의 이러한 집합들이 생기는데, 이를 합쳐서 oob(Out Of Bag)이라고 한다.\n",
        "rf = RandomForestClassifier(oob_score=True, n_jobs=-1)\n",
        "\n",
        "rf.fit(train_input, train_target)\n",
        "print(rf.oob_score_)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8943621319992303\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJU-I5E6jWII"
      },
      "source": [
        "# **엑스트라 트리**\n",
        "*엑스트라 트리:랜덤 포레스트 모델에서 부트스트랩 샘플을 사용하지 않고 노드분할을 무작위로 하여 성장을 규제하는 모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItYh6tK4jZ-N",
        "outputId": "dd020751-3e28-4deb-d9db-21e2fedfd68b"
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "#sklearn.ensemble패키지 하위에 기본적으로 포함되어있는 엑스트라 트리 모델인 ExtraTreeClassifier클래스를 사용한다.\n",
        "#랜덤하게 노드를 분할하기 때문에 랜덤 포레스트 모델보다 속도가 빠르다.\n",
        "#기본적으로 100개의 트리모델을 학습시킨다.\n",
        "\n",
        "et = ExtraTreesClassifier(n_jobs=-1)\n",
        "scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9978352895600912 0.8860840675205448\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yd22O4UCmEcJ",
        "outputId": "e0ff85cc-2c19-4f9c-8091-ba8949464ef0"
      },
      "source": [
        "et.fit(train_input, train_target)\n",
        "#[alchor sugar pH]\n",
        "print(et.feature_importances_)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.18661911 0.52688588 0.28649501]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMrfV7h9nXok"
      },
      "source": [
        "# **그레이디언트 부스팅**\n",
        "*그레이디언트 부스팅:회귀모델일 경우 평균제곱오차 손실함수를, 분류모델일 경우 로지스트 손실함수를 활용하여, 손실함수의 함수값을 낮추는 방향의 트리를 추가하여 학습하는 모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vt4j4wPsnybX",
        "outputId": "76b5df53-5dd2-43a2-9394-e19648fe8521"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "#sklearn.ensemble패키지 하위에 기본적으로 포함되어있는 그레이디언트 부스팅 모델인 GradientBoostingClassifier를 사용한다.\n",
        "#그레이디언트 부스팅 모델은 손실함수가 낮아지는 방향으로 트리를 순차적으로 추가하기때문에 앞의 두 모델보다 학습속도가 느리다.\n",
        "\n",
        "gb = GradientBoostingClassifier()\n",
        "scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8866169860396467 0.8703074331827942\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BILmSBYDoZdZ",
        "outputId": "fb09cae1-4f67-46eb-aba6-1742da152f7c"
      },
      "source": [
        "#n_estimators:트리의 개수를 지정\n",
        "#learning_rate:학습 비율(기본값:0.1). 커질수록 한번 학습시 모델에 반영하는 비율이 커진다.\n",
        "gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2)\n",
        "scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9416968521370734 0.876077774487303\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOMRe34mo5us",
        "outputId": "a32a6c70-b8c9-484e-de4a-f63dc0ce1818"
      },
      "source": [
        "gb.fit(train_input, train_target)\n",
        "#[alchor sugar pH]\n",
        "print(gb.feature_importances_)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.16595299 0.67431535 0.15973166]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUBgnQpTpAcb"
      },
      "source": [
        "# **히스토그램 기반의 그레이디언트 부스팅**\n",
        "*히스토그램 기반의 그레이디언트 부스팅:데이터를 몇 개의 구간으로 나누어 학습하는 그레이디언트 부스팅 알고리즘"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPAZfZzMpjoh",
        "outputId": "27be62d6-de7f-4361-8240-0506560d2c58"
      },
      "source": [
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "#sklearn.ensemble패키지 하위에 기본적으로 포함되어있는 히스토그램 기반의 그레이디언트 부스팅 모델인 HistGradientBoostingClassifier 사용한다.\n",
        "#하지만 사이킥런에 추가된지 얼마 안됬으며, 실험적인 측면의 모델이기 때문에, 이를 사용할때는 반드시 sklearn.experimental패키지 하위에 포함되어있는\n",
        "#enable_hist_gradient_boosting클래스를 임포트해줘야한다.\n",
        "\n",
        "#HistGradientBoostingClassifier모델은 기본적으로 데이터를 256개의 구간으로 나누는데\n",
        "#255개의 구간은 정상적인 데이터 값의 구간인데 비해 마지막 한구간은 누락된 데이터를 보관하는 구간이다.\n",
        "#따라서 판다스 등으로 데이터를 다운받아 테스트할때, 누락된 값이 있어도 전처리 할 필요가 없다.\n",
        "\n",
        "hgb = HistGradientBoostingClassifier()\n",
        "scores = cross_validate(hgb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "#HistGradientBoostingClassifier모델에는 특성중요도가 없다."
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9325088519940576 0.8810799955578588\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A9a8UQjuJfL"
      },
      "source": [
        "**permutation importance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gBNSNk-r8GF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84bfd209-c8a3-4651-cc43-caa8a103017f"
      },
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "#permutation importance(치환 중요도):각 특성값을 섞어서 성능이 가장 덜 하락한 특성 순서로 매긴 중요도.\n",
        "#ex)[alchor,sugar,ph]인 데이터[[2,15,8],  인 가상의 데이터를 1.[[8,15,8], 2.[[2,150,8], 3.[[2,15,9],  으로 바꾸어\n",
        "#                            [4,49,6],                   [2,49,6],    [4,15,6],     [4,49,8],\n",
        "#                            [8,150,9]]                  [4,150,9]]   [8,49,9]]     [8,150,6]]\n",
        "#원본 데이터와의 정확도를 비교하여 각 특성의 중요도를 검사한다.\n",
        "#예를 들어, 원본 데이터의 정확도가 80%가 나오고 1번의 데이터(alchor값을 뒤바꾼 데이터)가 정확도 50%, 2번 데이터(sugar의 값을 뒤바꾼 데이터)가 정확도 70%가 나왔다면\n",
        "#alchor보다 sugar가 더 중요한 값이 된다.\n",
        "\n",
        "#permutation_importance는 분류,회귀 모델 상관없이 모든 모델에 적용이 가능한 클래스이다.\n",
        "\n",
        "hgb.fit(train_input, train_target)\n",
        "#n_repeats:각 특성당 몇번 섞어 결괏값을 낼건지 결정\n",
        "#n_jobs:몇개의 코어를 사용할지 설정. -1일경우 가능한 모든 코어를 사용한다.\n",
        "result = permutation_importance(hgb, train_input, train_target, n_repeats=10, n_jobs=-1)\n",
        "print(result.importances_mean)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.09328459 0.25039446 0.08262459]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tkKXph6YHtj",
        "outputId": "bcbcfbb1-aaf8-4bfd-abff-b99e8bf597ab"
      },
      "source": [
        "#permutation_importance에 테스트세트를 넣고 검증할경우, 이 모델을 실전에 투입했을때 어떤 특성에 가중치가 붙는지 실험할 수 있다.\n",
        "result = permutation_importance(hgb, test_input, test_target, n_repeats=10, n_jobs=-1)\n",
        "print(result.importances_mean)\n",
        "#훈련세트를 넣었을 때와 마찬가지로 'sugar'특성의 중요도가 높음을 알 수 있다."
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.054      0.19692308 0.04384615]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUqb1_puYrH2"
      },
      "source": [
        "**XGBoost**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWSdwBI0YxVo",
        "outputId": "36f509eb-bf76-4476-e12b-7c701ce17468"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "#tree_method='hist'라고 하면 XGBoost모델이 히스토그램기반으로 학습한다.\n",
        "xgb = XGBClassifier(tree_method='hist')\n",
        "scores = cross_validate(xgb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8800265736277282 0.8681896424076406\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2hCvzCQZ4Ug"
      },
      "source": [
        "**LightGBM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XTER_gWZ9pL",
        "outputId": "af378c7a-4f45-4a4a-db86-02b6a9be3842"
      },
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "lgb = LGBMClassifier()\n",
        "scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9316910123260858 0.8816587695269119\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}